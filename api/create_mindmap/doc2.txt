# Sequence to Sequence basic

Task to make a machine translation

Input is a sequence: x1,x2,‚Ä¶xn

Output is also a sequence: y1,y2,‚Ä¶,yn

Note that the lengths could be different3

 the target sequence that maximizes the conditional probability:

$y^{\ast}=\arg\max\limits_{y}p(y|x).$

To define a machine translation system, we need to answer three questions:

- modeling¬†- how does the model for¬†¬†look like?
    
    $p(y|x,Œ∏)$
    
- learning¬†- how to find the parameters¬†?
    
    $Œ∏$
    
- inference¬†- how to find the best¬†?
    
    $y$
    

## Encoder-Decoder Framework

encoder-decoder is the standard modeling paradigm for seq2seq tasks. This framework consists of two components:

- what is encoder?
- what is decoder?

## Conditional Language Model

Why seq2seq taks could be modeled as Conditional Language Model (CLM)?

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/fc463398-109f-44c2-8b20-51b985d44cb8/image.png)

What is the difference from LMs from CDM?

- the high-level pipeline is as follow
    - feed source and previously generated target words into a network
    - get vector representation of context (both source and previous target) from the networks decoders
    - From this vector repre_, predict a probability distribution for the next token.
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/dbad7e8f-86be-451b-b3aa-68648d9936a7/image.png)
    

## The Simplest Model: Two RNNs for Encoder and Decoder

- This simplest encoder-decoder model consist of what?
- What is ‚Äúencoder RNN‚Äù do in the model?
- What is ‚Äúdecoder RNN‚Äù do in the model

Representations of sentences with similar meaning but different structure are close!

## Training: The cross-entropy loss

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/54047081-b2f8-4a0e-862d-995e8a816314/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/2906ffeb-4dec-4701-a9ec-66cd78b71529/image.png)

## Inference: greedy decoding and beam search

Let‚Äôs think how to generate a translation using this model. We model the probability of a sentence as follows:

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/559e00b3-efc6-4dca-a822-773708dfdb15/image.png)

Yes. How to find the argmax?

### Greedy decoding: at each step, pick the most probable token

What do we do in here? describe it verbally

Why is this method inherently flawed?

### Beam search: keep track of several most probably hypotheses

What do we do in here? describe it verbally

# Attention

## The problem of fixed encoder representation

why fixed source representation is suboptimal?

- To enoder?
- To decoder?

## Attention: A High-Level View

> Attention: At different steps, let a model ‚Äúfocus‚Äù on different parts of the input.
> 

describe verbally based on the image

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/822c619d-9a52-4010-b74a-457772dfd6f1/image.png)

## How to compute attention score?

What are the 3 most popular ways to compute attention scores?

- a
- b
- c

# Transformer: Attention is All You Need

The illustration from the Google AI blog post introducing Transformer

![](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif)

### Self-Attention: The ‚ÄúLook at Each Other‚Äù Part

What is the difference between Self-atention and attention?

Decoder-encoder attention is looking

- from: one current decoder state
- at: all encoder states

Self-attention is looking

- from: ?
- at: ?

Describe self-attention verbally.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/9b3e70c5-38e5-43ae-9625-cccc4006c925/image.png)

## Query, Key and Value in Self-attention

Each input token in self-attention receives three representations corresponding to the roles it can play:

- Query - asking for information
- Key - saying that it has some information
- value - giving the information

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/2c1bdfc9-f31b-4128-a8fe-07eda339f77a/image.png)

The formula for computing attention output is as follows:

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/f818913c-988c-464d-82d8-3aae1a2fcf87/image.png)

## Mask self-attention

What do we do in mask self-attention?

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/c166aa3d-b032-4f5e-b77e-4ba085c14446/image.png)

**How can the decoder do that? (do the question above)**

Because in training, we use reference translation (which we know). Therefore, in training, we feed the whole target sentence to the decoder - without masks, the tokens would ‚Äúsee future‚Äù, and this is not what we want

## Multi-head attention: Independently focus on different things.

<aside>
üí°

Each word is part of many relations

</aside>

‚áí We have to let the model focus on different things: this is the motivation behind **Multi-Head Attention**

https://lena-voita.github.io/resources/lectures/seq2seq/transformer/multi_head.mp4

T·ª©c l√† thay v√¨ ch·ªâ c√≥ m·ªôt input th√¨ b√¢y gi·ªù c√≥ nhi·ªÅu h∆°n 1 xong r·ªìi g·ªôp l·∫°i

# Transformer: Model Architecture

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/9af99d9d-280a-43e2-b136-b266748ea377/image.png)

Let‚Äôs look in more detail at the other model components

## Feed-forward blocks

Each layer has a feed-forward network blocks: two linear layers with ReLU non-linearity between them

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/3a3d6d45-c4ac-4ca4-ae48-5c1bcb443edd/image.png)

$FFN(x) = \max(0, xW_1+b_1)W_2+b_2.$

FFN - ‚Äútake a moment to think and process this information

## Residual connections

just add a block‚Äôs input to its output: they ease the gradient flow through a network and allow stacking a lot of layers

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/5fd3c6c9-d0b8-46ba-8ba9-fa2b43cd81d2/image.png)

There are used after each attention and FFN block.

## Layer Normalization

The ‚ÄúNorm‚Äù part in the ‚ÄúAdd & Norm‚Äù layer.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/052dc3ee-8e4d-4bf2-9103-c3cc894175fa/image.png)

It independently normalizes vector representation of each example in batch - this is done to control "flow" to the next layer. Layer normalization improves convergence stability and sometimes even quality.

Scale and bias are used after normalization to rescale layer‚Äôs outputs (or the next layer‚Äôs inputs).

u_k and o_k are evaluated for each example

## Positional encoding

**Problem:** Note that since Transformer does not contain recurrence or convolution, it does not know the order of input tokens. 

**Approach:** Therefore, we have to let the model know the positions of the tokens explicitly. For this, we have two sets of embeddings:

- Tokens: as we always do
- positions: the new ones needed for this model

Then input representation of a token is the **SUM OF TWO EMBEDDING: token and positional.**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/8bd5d47b-2bb7-41fb-8a9d-18491fa24082/image.png)

The positional embeddings can be learned, but the authors found that having fixed ones does not hurt the quality. The fixed positional encodings used in the Transformer are:

- ${PE}_{pos, 2i}=\sin(pos/10000^{2i/d{model}}),$
- ${PE}_{pos, 2i+1}=\cos(pos/10000^{2i/d{model}}),$

# subword segmentation: BPE

The part will teach you to cover the UNK vocab problems, where there are too many words that are not learnt yet.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/1bc216bb-bb76-4f15-9b48-c2b7a5dcb962/f54c6c60-6d37-48f8-9542-ba5d3e6c65ac/image.png)

subword - the part that you are familiar with but not the full word (e.g 

## how does it work?

BPE consists of two parts:

- training - learning ‚ÄúBPE rules‚Äù, which pairs of symbols to merge
- inference - apply rules to segment a text

### Training

The initial vocabulary consists of characters and an empty merge table

- count pairs of symbols: how many times each pair occurs together in the training data;
- find the most frequent pair of symbols;
- merge this pair - add a merge to the merge table, and the new token to the vocabulary.
if found a new word, we add the special characters¬†**@@**¬†to distinguish between tokens that represent entire words and tokens that represent parts of words. In our example,¬†**mat**¬†and¬†**mat@@**¬†are different tokens.


### **Inference: segment a text**

The algorithm starts with segmenting a word into a sequence of characters. After that, it iteratively makes the following two steps until no merge it possible:

- among all possible merges at this step, find the highest merge in the table;
- apply this merge.
